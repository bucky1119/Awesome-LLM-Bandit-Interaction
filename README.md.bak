
<div align="center">
<h2> A Survey on Recent Advances in the Connection between Large Language Models and Multi-armed Bandits </h2> 
<a href='https://arxiv.org/'><img src='https://img.shields.io/badge/Paper-blue
</div>

# üöÄ Introduction
- This repository provides a curated collection of papers and resources on the intersection of Large Language Models (LLMs) and Multi-Armed Bandits (MABs). As modern LLMs grow more capable, bandit algorithms offer efficient tools for improving prompt design, training, inference optimization, and personalization. Conversely, LLMs enable more expressive and context-aware bandit decision-making.

- Our goal is to give researchers a clear entry point into this emerging area, covering both LLM-enhanced bandits and bandit-enhanced LLMs, along with key insights, methods, and open challenges.
  <!-- <p align="center">
  <img src="assets/fig.jpg" width=300""> -->
<!-- </p> -->

- We welcome any contributions and suggestions to our repository or the addition of your own work. Feel free to make a pull request or leave your comments!!



# üìã Contents
- [üöÄ Introduction](#üöÄ-introduction)
- [üìã Contents](#üìã-contents)
- [üíò Tips](#üíò-tips)
- [üîç Search Strategy](#üîç-search-strategy)
- [üìç Bandit-enhancements for LLMs](#üìç-bandit-enhancements-for-llms)
  - [Pre-training](#pre-training)
  - [Fine-tuning](#fine-tuning)
  - [Prompt Design and Selection](#prompt-design-and-selection)
  - [Tool and Function Calling](#tool-and-function-calling)
  - [Contextual Understanding](#contextual-understanding)
  - [Retrieval-Augmented Generation](#retrieval-augmented-generation)
  - [Inference and Generation Optimization](#inference-and-generation-optimization)
  - [Adaptation and Personalizing](#adaptation-and-personalizing)
  
- [üìç LLM-enhancements for Multi-armed Bandits](#üìç-llm-enhancements-for-multi-armed-bandits)
  - [Optimization Objective](#optimization-objective)
  - [Arm Definition](#arm-definition)
  - [Environment](#environment)
  - [Reward Formulation](#reward-formulation)
  - [Sampling Strategy](#sampling-strategy)
  - [Action Decision](#action-decision)
- [üë®‚Äçüíª Team](#üë®‚Äçüíª-team)
- [üòâ Citation](#üòâ-citation)
<!-- - [‚≠êÔ∏è Star History](#‚≠êÔ∏è-star-history) -->


# üíò Tips
- **‚úÖ Paper searching via catatogue**: directly clicking the content of the catatogue to select the area of your research and browse related papers.
- **‚úÖ Paper searching via author name**: Free feel to search papers of a specific author via `ctrl + F` and then type the author name. The dropdown list of authors will automatically expand when searching.
- **‚úÖ Paper searching via tag**: You can also search the related papers via the following tags: `customization`, `iteractive`, `human motion generation` `tokenizer`. (More tags are ongoing)


# üîç Search Strategy

This repository is curated following a systematic methodology to ensure comprehensive and reproducible coverage of research at the intersection of Large Language Models (LLMs) and Bandit Algorithms.

## 1. Search Framework
We adopted the **PCC (Population, Concept, Context)** framework as recommended in 'The Systematic Review: An Overview' by *Aromataris & Pearson (2014)* to structure our search strings.

## 2. Search Queries 
Our search strategy is organized into four hierarchical layers to balance **Sensitivity** (finding all relevant papers) and **Specificity** (filtering out noise).

### A. Core Intersection (High-level)
*Broad terms used to identify foundational literature at the cross-section.*
1. `"Large Language Model" AND "Bandit"`
2. `"LLM" AND "Multi-armed Bandit"`
3. `"Generative AI" AND "Contextual Bandit"`
4. `"Foundation Model" AND "Sequential Decision Making"`
5. `"Autoregressive Model" AND "Online Learning"`

### B. LLM Component-Based Search
*Focusing on where Bandit algorithms enhance specific LLM stages.*

6. **Pre-training:**
   `("Pre-training" AND ("Bandit" OR "Exploration"))`

7. **Fine-tuning:**
   `("Fine-tuning" AND ("Bandit" OR "Online optimization"))`

8. **Alignment:**
   `(("Alignment" OR "RLHF" OR "Preference learning") AND "Bandit")`

9. **Prompt Design and Selection:**
   `(("Prompt selection" OR "Prompt optimization" OR "In-context learning") AND "Bandit")`

10. **Tool and Function Calling:**
    `(("Tool calling" OR "Function calling" OR "Action invocation") AND "Bandit")`

11. **Context Understanding:**
    `(("Context management" OR "Long context" OR "Context window") AND "Bandit")`

12. **Retrieval-Augmented Generation (RAG):**
    `(("Retrieval-augmented generation" OR "RAG") AND "Bandit")`

13. **Inference Optimization:**
    `(("Inference optimization" OR "Resource allocation") AND "Bandit")`

14. **Decoding Strategies:**
    `(("Decoding strategy" OR "Sampling strategy" OR "Generation control") AND "Bandit")`

15. **Adaptation and Personalization:**
    `(("Personalization" OR "Model adaptation" OR "User preference modeling") AND "Bandit")`

### C. Bandit Component-Based Search
*Focusing on how LLMs are integrated into Bandit framework elements.*

16. **Regret Minimization Objective:**
    `(("Regret minimization" OR "Reward maximization") AND "LLM")`

17. **Arm Definition:**
    `(("Action space" OR "Arm representation") AND "LLM")`

18. **Environment Modeling:**
    `(("Environment modeling" OR "Dynamic environment" OR "Simulator") AND "LLM")`

19. **Reward Formulation:**
    `(("Reward modeling" OR "Reward shaping") AND "LLM")`

20. **Sampling Strategy:**
    `(("Thompson Sampling" OR "UCB" OR "Exploration strategy") AND "LLM")`

21. **Action Decision:**
    `(("Action selection" OR "Decision making") AND "LLM")`

### D. Specialized & Emerging Interaction Terms
*Capturing specific techniques and interdisciplinary applications.*

22. `"RLHF" AND "Bandit"`
23. `"Combinatorial Bandit" AND "Text Generation"`
24. `"Neural Bandit" AND "Transformer"`
25. `"Bayesian Optimization" AND "LLM"`
26. `"Active Learning" AND "LLM" AND "Bandit"`
27. `"Interactive NLP" AND "Bandit"`
28. `"Resource allocation" AND "LLM inference" AND "Bandit"`
29. `"Query selection" AND "RAG" AND "Bandit"`
30. `"Automated prompt engineering" AND "Multi-armed Bandit"`
31. `"Speculative decoding" AND "Multi-armed Bandit"`
32. `"User preference modeling" AND "LLM" AND "Bandit"` 


# üìç Bandit-enhancements for LLMs

## Pre-training

+ **Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking** (2022)<details><summary>Inigo Urteaga, Moulay Zaidane Draidia, Tomer Lancewicki, et al.</summary>Inigo Urteaga, Moulay Zaidane Draidia, Tomer Lancewicki, Shahram Khadivi</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2203.13151)


+ **Efficient online data mixing for language model pre-training** (2023)<details><summary>Alon Albalak, Liangming Pan, Colin Raffel, et al.</summary>Alon Albalak, Liangming Pan, Colin Raffel, William Yang Wang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2312.02406)

 
+ **Pretraining Decision Transformers with Reward Prediction for In-Context Multi-task Structured Bandit Learning** (2024)<details><summary>Subhojyoti Mukherjee, Josiah P Hanna, Qiaomin Xie, et al.</summary>Subhojyoti Mukherjee, Josiah P Hanna, Qiaomin Xie, Robert Nowak</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2406.05064)


+ **Harnessing Diversity for Important Data Selection in Pretraining Large Language Models** (2024)<details><summary>Chi Zhang, Huaping Zhong, Kuan Zhang, et al.</summary>Chi Zhang, Huaping Zhong, Kuan Zhang, Chengliang Chai, Rui Wang, Xinlin Zhuang, Tianyi Bai, Jiantao Qiu, Lei Cao, Ye Yuan, others</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2409.16986)


+ **Actor-Critic based Online Data Mixing For Language Model Pre-Training** (2025)<details><summary>Jing Ma, Chenhao Dang, Mingjie Liao</summary>Jing Ma, Chenhao Dang, Mingjie Liao</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2505.23878)


+ **EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration** (2025)<details><summary>Allen Nie, Yi Su, Bo Chang, et al.</summary>Allen Nie, Yi Su, Bo Chang, Jonathan Lee, Ed H. Chi, Quoc V Le, Minmin Chen</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2410.06238)



## Fine-tuning


+ **RL-NMT: Reinforcement Learning Fine-tuning for Improved Neural Machine Translation of Burmese Dialects** (2023)<details><summary>Ye Kyaw Thu, Thazin Myint Oo, Thepchai Supnithi</summary>Ye Kyaw Thu, Thazin Myint Oo, Thepchai Supnithi</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.1808.08866)


+ **Reflect-RL: Two-Player Online RL Fine-Tuning for LMs** (2024)<details><summary>Runlong Zhou, Simon S Du, Beibin Li</summary>Runlong Zhou, Simon S Du, Beibin Li</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2402.12621)


+ **Dynamic Data Mixing Maximizes Instruction Tuning for Mixture-of-Experts** (2024)<details><summary>Tong Zhu, Daize Dong, Xiaoye Qu, et al.</summary>Tong Zhu, Daize Dong, Xiaoye Qu, Jiacheng Ruan, Wenliang Chen, Yu Cheng</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2406.11256)


+ **Iterative data smoothing: Mitigating reward overfitting and overoptimization in rlhf** (2024)<details><summary>Banghua Zhu, Michael I Jordan, Jiantao Jiao</summary>Banghua Zhu, Michael I Jordan, Jiantao Jiao</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2401.16335)


+ **Sharp Analysis for KL-Regularized Contextual Bandits and RLHF** (Unknown Year)<details><summary>Heyang Zhao, Chenlu Ye, Quanquan Gu, et al.</summary>Heyang Zhao, Chenlu Ye, Quanquan Gu, Tong Zhang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2411.04625)


+ **Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits** (2024)<details><summary>Yu Xia, Fang Kong, Tong Yu, et al.</summary>Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A Rossi, Sungchul Kim, Shuai Li</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2403.07213)

+ **Convergence-aware online model selection with time-increasing bandits** (2024)<details><summary>Yu Xia, Fang Kong, Tong Yu, et al.</summary>Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A Rossi, Sungchul Kim, Shuai Li</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2403.07213)


+ **Preference fine-tuning of LLMs should leverage suboptimal, on-policy data** (2024)<details><summary>Fahim Tajwar, Anikait Singh, Archit Sharma, et al.</summary>Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, Aviral Kumar</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2404.14367)


+ **Sample-efficient alignment for llms** (2024)<details><summary>Zichen Liu, Changyu Chen, Chao Du, et al.</summary>Zichen Liu, Changyu Chen, Chao Du, Wee Sun Lee, Min Lin</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2411.01493)


+ **Chunks as arms: Multi-armed bandit-guided sampling for long-context llm preference optimization** (2025)<details><summary>Shaohua Duan, Xinze Li, Zhenghao Liu, et al.</summary>Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2508.13993)


+ **DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections** (2025)<details><summary>Haebin Shin, Lei Ji, Xiao Liu, et al.</summary>Haebin Shin, Lei Ji, Xiao Liu, Zhiwei Yu, Qi Chen, Yeyun Gong</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2508.12116)


## Alignment


## Prompt Design and Selection


+ **Prompt Optimization with Human Feedback** (2024)<details><summary>Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, et al.</summary>Xiaoqiang Lin, Zhongxiang Dai, Arun Verma, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2405.17346)


+ **Best arm identification for prompt learning under a limited budget** (2024)<details><summary>Chengshuai Shi, Kun Yang, Jing Yang, et al.</summary>Chengshuai Shi, Kun Yang, Jing Yang, Cong Shen</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2209.07330)


+ **Prompt optimization with EASE? efficient ordering-aware automated selection of exemplars** (2024)<details><summary>Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, et al.</summary>Zhaoxuan Wu, Xiaoqiang Lin, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2405.16122)


+ **Prompt Optimization with Logged Bandit Data** (2024)<details><summary>Haruka Kiyohara, Yuta Saito, Daniel Yiming Cao, et al.</summary>Haruka Kiyohara, Yuta Saito, Daniel Yiming Cao, Thorsten Joachims</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2504.02646)


+ **Prompt-based Code Completion via Multi-Retrieval Augmented Generation** (2024)<details><summary>Hanzhuo Tan, Qi Luo, Ling Jiang, et al.</summary>Hanzhuo Tan, Qi Luo, Ling Jiang, Zizheng Zhan, Jing Li, Haotian Zhang, Yuqun Zhang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2405.07530)


+ **Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers** (2024)<details><summary>Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, et al.</summary>Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2310.02905)


+ **Efficient prompt optimization through the lens of best arm identification** (2024)<details><summary>Chengshuai Shi, Kun Yang, Zihan Chen, et al.</summary>Chengshuai Shi, Kun Yang, Zihan Chen, Jundong Li, Jing Yang, Cong Shen</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2402.09723)


+ **FedPOB: Sample-Efficient Federated Prompt Optimization via Bandits** (2025)<details><summary>Pingchen Lu, Zhi Hong, Zhiwei Shang, et al.</summary>Pingchen Lu, Zhi Hong, Zhiwei Shang, Zhiyong Wang, Yikun Ban, Yao Shu, Min Zhang, Shuang Qiu, Zhongxiang Dai</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2509.24701)


+ **Prompt Tuning Decision Transformers with Structured and Scalable Bandits** (2025)<details><summary>Finn Rietz, Oleg Smirnov, Sara Karimi, et al.</summary>Finn Rietz, Oleg Smirnov, Sara Karimi, Lele Cao</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2003.02287)


+ **Meta-prompt optimization for llm-based sequential decision making** (2025)<details><summary>Mingze Kong, Zhiyong Wang, Yao Shu, et al.</summary>Mingze Kong, Zhiyong Wang, Yao Shu, Zhongxiang Dai</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2502.00728)


+ **Bandit-Based Prompt Design Strategy Selection Improves Prompt Optimizers** (2025)<details><summary>Rin Ashizawa, Yoichi Hirose, Nozomu Yoshinari, et al.</summary>Rin Ashizawa, Yoichi Hirose, Nozomu Yoshinari, Kento Uchida, Shinichi Shirakawa</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2503.01163)


+ **Prompt Optimization with Logged Bandit Data** (2025)<details><summary>Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, et al.</summary>Haruka Kiyohara, Daniel Yiming Cao, Yuta Saito, Thorsten Joachims</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2504.02646)


+ **LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization** (2025)<details><summary>Yuanchen Wu, Saurabh Verma, Justin Lee, et al.</summary>Yuanchen Wu, Saurabh Verma, Justin Lee, Fangzhou Xiong, Poppy Zhang, Amel Awadelkarim, Xu Chen, Yubai Yuan, Shawndra Hill</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2510.13907)

## Tool and Function Calling

## Contextual Understanding
+ **Can large language models explore in-context?** (2024)<details><summary>Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, et al.</summary>Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, Cyril Zhang, Aleksandrs Slivkins</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2403.15371)


+ **Code Repair with LLMs gives an Exploration-Exploitation Tradeoff** (2024)<details><summary>Hao Tang, Keya Hu, Jin Peng Zhou, et al.</summary>Hao Tang, Keya Hu, Jin Peng Zhou, Sicheng Zhong, Wei-Long Zheng, Xujie Si, Kevin Ellis</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2405.17503)


+ **Efficient exploration for llms** (2024)<details><summary>Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, et al.</summary>Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2402.00396)


+ **Enhancing Sequential Recommendations through Multi-Perspective Reflections and Iteration** (2024)<details><summary>Weicong Qin, Yi Xu, Weijie Yu, et al.</summary>Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Xiao Zhang, Ming He, Jianping Fan, Jun Xu</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2409.06377)


+ **Controlling Large Language Model Agents with Entropic Activation Steering** (2024)<details><summary>Nate Rahn, Pierluca D'Oro, Marc G Bellemare</summary>Nate Rahn, Pierluca D'Oro, Marc G Bellemare</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2406.00244)


+ **Chunks as arms: Multi-armed bandit-guided sampling for long-context llm preference optimization** (2025)<details><summary>Shaohua Duan, Xinze Li, Zhenghao Liu, et al.</summary>Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2508.13993)


+ **Efficient jailbreak attack sequences on large language models via multi-armed bandit-based context switching** (2025)<details><summary>Aditya Ramesh, Shivam Bhardwaj, Aditya Saibewar, et al.</summary>Aditya Ramesh, Shivam Bhardwaj, Aditya Saibewar, Manohar Kaul</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2410.11533)


+ **EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration** (2025)<details><summary>Allen Nie, Yi Su, Bo Chang, et al.</summary>Allen Nie, Yi Su, Bo Chang, Jonathan Lee, Ed H. Chi, Quoc V Le, Minmin Chen</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2410.06238)


+ **Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution** (2025)<details><summary>Manhin Poon, XiangXiang Dai, Xutong Liu, et al.</summary>Manhin Poon, XiangXiang Dai, Xutong Liu, Fang Kong, John Lui, Jinhang Zuo</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2506.17670)


+ **Context Attribution with Multi-Armed Bandit Optimization** (2025)<details><summary>Deng Pan, Keerthiram Murugesan, Nuno Moniz, et al.</summary>Deng Pan, Keerthiram Murugesan, Nuno Moniz, Nitesh Chawla</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2506.19977)


## Retrieval-Augmented Generation
+ **M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions** (2024)<details><summary>Zheng Wang, Shu Xian Teo, Jieer Ouyang, et al.</summary>Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, Wei Shi</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2405.16420)


+ **AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation** (2024)<details><summary>Jia Fu, Xiaoting Qin, Fangkai Yang, et al.</summary>Jia Fu, Xiaoting Qin, Fangkai Yang, Lu Wang, Jue Zhang, Qingwei Lin, Yubo Chen, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2406.19251)


+ **Advances in Neural Information Processing Systems** (2024)<details><summary>Qinggang Zhang, Junnan Dong, Hao Chen, et al.</summary>Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, Xiao Huang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://wenku.baidu.com/view/be3011116edb6f1aff001f00.html?fr=sogou&_wkts_=1763891239715)


+ **MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity** (2025)<details><summary>Xiaqiang Tang, Qiang Gao, Jian Li, et al.</summary>Xiaqiang Tang, Qiang Gao, Jian Li, Nan Du, Qi Li, Sihong Xie</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2412.01572)


+ **IEEE INFOCOM 2025 - IEEE Conference on Computer Communications** (2025)<details><summary>Tao Ouyang, Guihang Hong, Kongyange Zhao, et al.</summary>Tao Ouyang, Guihang Hong, Kongyange Zhao, Zhi Zhou, Weigang Wu, Zhaobiao Lv, Xu Chen</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11199746)


+ **Adapting to non-stationary environments: Multi-armed bandit enhanced retrieval-augmented generation on knowledge graphs** (2025)<details><summary>Xiaqiang Tang, Jian Li, Nan Du, et al.</summary>Xiaqiang Tang, Jian Li, Nan Du, Sihong Xie</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2412.07618)


+ **Context Attribution with Multi-Armed Bandit Optimization** (2025)<details><summary>Deng Pan, Keerthiram Murugesan, Nuno Moniz, et al.</summary>Deng Pan, Keerthiram Murugesan, Nuno Moniz, Nitesh Chawla</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2506.19977)

## Inference Optimization

## Decoding Strategies



## Inference and Generation Optimization

+ **Bandits don't follow rules: Balancing multi-facet machine translation with multi-armed bandits** (2021)<details><summary>Julia Kreutzer, David Vilar, Artem Sokolov</summary>Julia Kreutzer, David Vilar, Artem Sokolov</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2110.06997)


+ **Cost-efficient Knowledge-based Question Answering with Large Language Models** (2024)<details><summary>Junnan Dong, Qinggang Zhang, Chuang Zhou, et al.</summary>Junnan Dong, Qinggang Zhang, Chuang Zhou, Hao Chen, Daochen Zha, Xiao Huang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2405.17337)


+ **EVOLvE: Evaluating and Optimizing LLMs For Exploration** (2024)<details><summary>Allen Nie, Yi Su, Bo Chang, et al.</summary>Allen Nie, Yi Su, Bo Chang, Jonathan N Lee, Ed H Chi, Quoc V Le, Minmin Chen</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2410.06238)


+ **Sequential query prediction based on multi-armed bandits with ensemble of transformer experts and immediate feedback** (2024)<details><summary>Shameem A Puthiya Parambath, Christos Anagnostopoulos, Roderick Murray-Smith</summary>Shameem A Puthiya Parambath, Christos Anagnostopoulos, Roderick Murray-Smith</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://link.springer.com/article/10.1007/s10618-024-01057-4)


+ **Speculative Decoding via Early-exiting for Faster LLM Inference with Thompson Sampling Control Mechanism** (2024)<details><summary>Jiahao Liu, Qifan Wang, Jingang Wang, et al.</summary>Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2406.03853)


+ **Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits** (2024)<details><summary>Yu Xia, Fang Kong, Tong Yu, et al.</summary>Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A Rossi, Sungchul Kim, Shuai Li</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://dl.acm.org/doi/10.1145/3589334.3645420)


+ **Convergence-Aware Online Model Selection with Time-Increasing Bandits** (2024)<details><summary>Yu Xia, Fang Kong, Tong Yu, et al.</summary>Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A Rossi, Sungchul Kim, Shuai Li</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2403.07213)


+ **LLMs Are In-Context Bandit Reinforcement Learners** (2024)<details><summary>Giovanni Monea, Antoine Bosselut, Kiant\'e Brantley, et al.</summary>Giovanni Monea, Antoine Bosselut, Kiant\'e Brantley, Yoav Artzi</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2410.05362)


+ **Dynamic and Cost-Efficient Deployment of Large Language Models Using Uplift Modeling and Multi Armed Bandits** (2025)<details><summary>Ninad Tongay</summary>Ninad Tongay</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://nicsefc.ee.tsinghua.edu.cn/nics_file/pdf/f06a14c1-4d6d-441d-b4e4-82545ac5781b.pdf)


+ **Tokenized Bandit for LLM Decoding and Alignment** (2025)<details><summary>Suho Shin, Chenghao Yang, Haifeng Xu, et al.</summary>Suho Shin, Chenghao Yang, Haifeng Xu, Mohammad T Hajiaghayi</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2506.07276)


+ **Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs** (2025)<details><summary>Hongyi Liu, Jiaji Huang, Zhen Jia, et al.</summary>Hongyi Liu, Jiaji Huang, Zhen Jia, Youngsuk Park, Yu-Xiang Wang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2510.20064)


+ **LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference** (2025)<details><summary>Hantao Yang, Hong Xie, Defu Lian, et al.</summary>Hantao Yang, Hong Xie, Defu Lian, Enhong Chen</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2509.15515)


+ **BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms** (2025)<details><summary>Yunlong Hou, Fengzhuo Zhang, Cunxiao Du, et al.</summary>Yunlong Hou, Fengzhuo Zhang, Cunxiao Du, Xuan Zhang, Jiachun Pan, Tianyu Pang, Chao Du, Vincent YF Tan, Zhuoran Yang</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2505.15141)


+ **LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing** (2025)<details><summary>Yang Li</summary>Yang Li</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2502.02743)


+ **LLM-MS: A multi-model llm search engine** (2025)<details><summary>Konstantin Krasovitskiy, Stelios Christou, Demetrios Zeinalipour-Yazti</summary>Konstantin Krasovitskiy, Stelios Christou, Demetrios Zeinalipour-Yazti</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://ieeexplore.ieee.org/document/11108133)


+ **Learning to Route LLMs from Bandit Feedback: One Policy, Many Trade-offs** (2025)<details><summary>Wang Wei, Tiankai Yang, Hongjie Chen, et al.</summary>Wang Wei, Tiankai Yang, Hongjie Chen, Yue Zhao, Franck Dernoncourt, Ryan A. Rossi, Hoda Eldardiry</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2510.07429)


+ **Adaptive llm routing under budget constraints** (2025)<details><summary>Pranoy Panda, Raghav Magazine, Chaitanya Devaguptapu, et al.</summary>Pranoy Panda, Raghav Magazine, Chaitanya Devaguptapu, Sho Takemori, Vishal Sharma</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2508.21141)


+ **Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks** (2025)<details><summary>Baran Atalar, Eddie Zhang, Carlee Joe-Wong</summary>Baran Atalar, Eddie Zhang, Carlee Joe-Wong</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://arxiv.org/abs/2508.09958)


+ **How Do We Select Right {LLM} for Each Query?** (2025)<details><summary>Bowen Zhang, Gang Wang, Qi Chen, et al.</summary>Bowen Zhang, Gang Wang, Qi Chen, Anton van den Hengel</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://openreview.net/forum?id=AfA3qNY0Fq)




## Adaptation and Personalization

+ **Personalizing natural language understanding using multi-armed bandits and implicit feedback** (2020)<details><summary>Fabian Moerchen, Patrick Ernst, Giovanni Zappella</summary>Fabian Moerchen, Patrick Ernst, Giovanni Zappella</details></details>
[![Paper](https://img.shields.io/badge/Paper-blue)](https://doi.org/10.48550/arXiv.2102.13101)


+ **User Feedback-based Online Learning for Intent Classification** (2023)<details><summary>Kaan G\"on
